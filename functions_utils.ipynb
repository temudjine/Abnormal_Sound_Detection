{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"eFemfgHCSkhS"},"outputs":[],"source":["def enregistrement_npy(dt=256):\n","  for machine in df.Machine.unique():\n","      #Génération de spectres\n","      df_train = df[ (df['Machine']== machine) & (df['train_test'] == 'train') ] # sélection de la machine \n","      df_valid = df[ (df['Machine']== machine) & (df['train_test'] == 'test') ] # sélection de la machine \n","\n","      #partie non testée pour adapter l'URL au google drive\n","      URLS_train=[]\n","      URLS_valid=[]\n","      for lignes in df_train['URL']:\n","        URLS_train.append(str(\"/content/drive/MyDrive/Datascience projet son/\"+lignes))\n","      for lignes in df_valid['URL']:\n","        URLS_valid.append(str(\"/content/drive/MyDrive/Datascience projet son/\"+lignes))\n","\n","      data_train = load_data(URLS_train, dt=dt, T_max=10,normalize=True)\n","      data_valid = load_data(URLS_valid, dt=dt, T_max=10,normalize=True)\n","      \n","      # on cherche la puissance de 2 la plus proche du nb fréquence\n","      carre=2\n","      while carre<data_train.shape[2]:\n","          carre*=2\n","      \n","      freq=int(carre/2)\n","      \n","      #Reduction de dimension\n","      sel=downsizing()      \n","      sel.fit(data_train,seuil_db=False,nb_freq=freq)\n","      data_train=sel.transform(data_train,normalize=False)\n","      data_valid=sel.transform(data_valid,normalize=False)\n","\n","      #Enregistrement\n","      nom_fichier_train=str(str(machine)+\"_train_norm_\"+str(dt)+\"_\"+str(freq)+\".npy\")\n","      nom_fichier_test=str(str(machine)+\"_test_norm_\"+str(dt)+\"_\"+str(freq)+\".npy\")\n","\n","      np.save(nom_fichier_train,data_train.flatten())\n","      np.save(nom_fichier_test,data_valid.flatten())\n","  return\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-lHE31qSkhc"},"outputs":[],"source":["def load_data(audio_path, dt=256, T_max=10, normalize=True):\r\n","    \"\"\"\r\n","    Permet de charger un spectre logmel depuis un path. \r\n","    \r\n","    params: \r\n","    \r\n","    dt: float /int. si [0:1[ alors dt est considéré comme un pas de découpage en seconde \r\n","        si >=1 alors doit être un int réprésentant le nombre de découpages souhaité\r\n","    \r\n","    \"\"\"\r\n","    X_audio=[]\r\n","    \r\n","    if dt<1: # on veut selectionner le pas en seconde\r\n","        with tqdm(total=len(audio_path)) as pbar: # code pour afficher une barre de chargement\r\n","            for i, a_path in enumerate(audio_path) :\r\n","                pbar.update(1) # permet de mettre la barre de chargement à jour en ajoutant 1 itération\r\n","                # Load the audio file\r\n","                data, fe = librosa.load(a_path, sr=None)\r\n","\r\n","                # For the audio > T_max : Use just the fist T_max seconde, to have the right shape. \r\n","                if len(data)>= T_max*fe:\r\n","                    data = data[:int(T_max*fe)]\r\n","\r\n","                # For the audio < T_max : Add in the signal a zeros vector, to have the right shape.\r\n","                else :\r\n","                    data = np.concatenate([data, np.zeros(int(T_max*fe - len(data)))])\r\n","\r\n","                # Apply the logMelSpectrogram function.    \r\n","                spectre_audio = logMelSpectrogram(data, fe, dt)\r\n","                if normalize : \r\n","                    spectre_audio=(np.array(spectre_audio)-np.array(spectre_audio).min())/(np.array(spectre_audio).max()-np.array(spectre_audio).min())\r\n","                    X_audio.append(spectre_audio)\r\n","            \r\n","        \r\n","    else: # on veut selectionner un nombre de dt \r\n","        with tqdm(total=len(audio_path)) as pbar:\r\n","            for a_path in audio_path :\r\n","                pbar.update(1)\r\n","                # Load the audio file\r\n","                data, fe = librosa.load(a_path, sr=None)\r\n","\r\n","                # For the audio > T_max : Use just the fist T_max seconde, to have the right shape. \r\n","                if len(data)>= T_max*fe:\r\n","                    data = data[:int(T_max*fe)]\r\n","\r\n","                # For the audio < T_max : Add in the signal a zeros vector, to have the right shape.\r\n","                else :\r\n","                    data = np.concatenate([data, np.zeros(int(T_max*fe - len(data)))])\r\n","\r\n","                # Apply the logMelSpectrogram function. \r\n","\r\n","                spectre_audio = logMelSpectrogram(data[:(len(data)//dt)*dt], fe, np.around(T_max/dt,3))\r\n","                spectre_audio=spectre_audio[:dt,:] # on tronque quand même au cas où nous avons dépasser le nombre de dt. \r\n","                if normalize : \r\n","                    spectre_audio=(np.array(spectre_audio)-np.array(spectre_audio).min())/(np.array(spectre_audio).max()-np.array(spectre_audio).min())\r\n","\r\n","                X_audio.append(spectre_audio)\r\n","                if np.array(X_audio).shape[1]!=dt:\r\n","                  raise Exception(\"Erreur le nombre de dt réalisé ne coincide pas avec la demande, changer le nombre demandé \\n\" \"format actuel de sortie\",np.array(X_audio).shape)\r\n","    \r\n","        return np.array(X_audio)\r\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VfMrk12ASkhd"},"outputs":[],"source":["class spectrum_generator():\n","    def __init__(self):\n","        self.level=0\n","        self.v_translate=0\n","        self.cut=None\n","        self.nb_freq=0\n","        self.nb_dt=0\n","        self.nb_samples_tot=0\n","        self.nb_samples_target=0\n","        \n","    def fit(self,X,y,level=0,v_translate=0,dt_cut=None):\n","        #récupération des paramètres\n","        self.level=level\n","        self.v_translate=v_translate\n","        self.nb_samples_tot,self.nb_dt,self.nb_freq=X.shape\n","        \n","        if dt_cut=='first':\n","            self.cut=0\n","        elif dt_cut=='last':\n","            self.cut=-1\n","        elif dt_cut=='both':  \n","            self.cut=[0,-1]\n","        else :\n","            self.cut=None\n","        \n","    def flow_from_numpy(self,X,y,batchsize=32,pourcent_modif=0.1):\n","        #initialisation de la selection aléatoire des samples\n","        selector=np.random.random(batchsize)*self.nb_samples_tot\n","        selector=selector.astype(int)\n","        transform_sel=np.random.random(int(pourcent_modif*batchsize))*selector.shape\n","        transform_sel=transform_sel.astype(int)\n","        #sélection des samples\n","        X_selected=np.array(X)[selector]\n","        y_selected=np.array(y)[selector]\n","        #transformation aléatoire des samples via une des fonctions\n","        for modif in transform_sel:\n","            choix_modif=int(np.random.random(1)*3)\n","            if choix_modif<=1:\n","                X_selected[modif,:,:]=X_selected[modif,:,:]*(1+(self.level*np.random.random(1))-(self.level*np.random.random(1))) # on multiplie le spectre par une valeur 1+- self.level\n","           \n","            elif (choix_modif>1) and (choix_modif<=2):\n","                X_selected[modif,:,:]=X_selected[modif,:,:]+(self.v_translate*np.random.random(1))-(self.v_translate*np.random.random(1))# on ajoute au spectre une valeur 1+- self.v_translate\n","            \n","            elif (choix_modif>2) and (choix_modif<=3):\n","                for dt in self.cut:\n","                    X_selected[modif,dt,:]=X_selected[modif,dt,:]*0\n","            else:\n","                X_selected[modif,:,:]=X_selected[modif,:,:]\n","        \n","        return X_selected,y_selected\n","        \n","    def add_samples(self,X,y,pourcent_add=0.1, labels='all'):\n","        self.nb_samples_tot=y.shape[0]\n","        if labels=='all':\n","            self.nb_samples_target=self.nb_samples_tot\n","        else:\n","            self.nb_samples_target=y[y==labels].shape[0]\n","        \n","        if self.nb_samples_target ==self.nb_samples_tot : \n","            #initialisation de la selection aléatoire des samples\n","            selector=np.random.random(int(pourcent_add*self.nb_samples_tot))*self.nb_samples_tot\n","            selector=selector.astype(int)\n","            #sélection des samples\n","            X_selected=np.array(X)[selector]\n","            y_selected=np.array(y)[selector]\n","        \n","        else :\n","            selector=np.random.random(int(pourcent_add*self.nb_samples_target))*self.nb_samples_target\n","            selector=selector.astype(int)\n","            #sélection des samples\n","            X_selected=np.array(X)[y==labels][selector]\n","            y_selected=np.array(y)[y==labels][selector]\n","        \n","        #transformation aléatoire des samples via une des fonctions\n","        for modif in range(X_selected.shape[0]):\n","            choix_modif=int(np.random.random(1)*3)\n","            if choix_modif<=1:\n","                X_selected[modif,:,:]=X_selected[modif,:,:]*(1+self.level)\n","           \n","            elif (choix_modif>1) and (choix_modif<=2):\n","                X_selected[modif,:,:]=X_selected[modif,:,:]*(1-self.level)  \n","            \n","            elif (choix_modif>2) and (choix_modif<=3):\n","                for dt in self.cut:\n","                    X_selected[modif,dt,:]=X_selected[modif,dt,:]*0\n","            else:\n","                X_selected[modif,:,:]=X_selected[modif,:,:]\n","        \n","        return np.concatenate((X,X_selected),axis=0),np.concatenate((y,y_selected),axis=0)   \n","        \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G1d9ohFiSkhg"},"outputs":[],"source":["class downsizing():\n","    def __init__(self):\n","        self.selector=0\n","        self.datamax=0\n","    \n","    def fit(self,X,seuil_db=False,nb_freq=False):\n","        self.datamax=X.max(axis=1).max(axis=0)\n","        if (seuil_db!=False) and (nb_freq!=False):\n","                return print('Renseignez un seuil_db OU un nombre de fréquences à garder mais pas les deux')\n","\n","        elif seuil_db=='brute':\n","            #Méthode brute\n","            data_s=X.sum(axis=1).sum(axis=0)\n","            data_s.shape\n","            self.selector=np.where(data_s>0,True,False)  \n","        elif seuil_db!=False:  \n","            self.selector=np.where(self.datamax>seuil_db,True,False)\n","\n","        elif nb_freq!=False:  \n","            self.selector=np.where(self.datamax>np.sort(self.datamax)[-(nb_freq+1)],True,False) # on est très restrictif dans un premier temps \n","\n","            if np.sum(self.selector)<nb_freq:\n","                self.selector=np.where(self.datamax>=np.sort(self.datamax)[-(nb_freq+1)],True,False) # si nous avons été trop restrictif, alors nous élargissons la selection puis nous bouclons pour supprimer le surplus\n","\n","            iter=100\n","            while np.sum(self.selector)>nb_freq and iter>0: # En cas de multiples valeurs similaires on boucle pour les supprimer et ne garder que la dim voulue\n","                print('Trop de fréquences choisies, suppression de {:} fréquences pour atteindre la taille cible'.format( self.selector.sum()-nb_freq))\n","                self.selector[np.argsort(np.sort(self.datamax[self.selector]))[-1]]=False  # on récupère la position du plus petit élément de datamax pour la supprimer du sélector\n","                iter-=1 # on met 10 itérations pour éviter de boucler à l'infini en cas d'erreur\n","        print('nombre de fréquences gardées: ', self.selector.sum())\n","        \n","    def transform(self,X,normalize=False):\n","        \n","        if normalize:\n","            def normalisation(ligne2D):\n","                return (np.array(ligne2D)-np.array(ligne2D).min())/(np.array(ligne2D).max()-np.array(ligne2D).min())  \n","            \n","            X_select=X[:,:,self.selector]\n","            X_select=np.array([ normalisation(samples) for samples in X_select ])\n","            return X_select\n","        \n","        else :\n","            return X[:,:,self.selector]\n","    \n","    \n","    def get_selector(self,seuil_db):\n","        self.selector=np.where(self.datamax>seuil_db,True,False)\n","        return self.selector\n","    \n","    \n","    def get_datamax(self):\n","        return self.datamax\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wOIBiY8fSkhj"},"outputs":[],"source":["def logMelSpectrogram(audio, fe, dt):\n","    stfts = np.abs(librosa.stft(audio,n_fft = int(dt*fe),hop_length = int(dt*fe),center = True)).T\n","    num_spectrogram_bins = stfts.shape[-1]\n","    # MEL filter\n","    linear_to_mel_weight_matrix = librosa.filters.mel(sr=fe,n_fft=int(dt*fe) + 1,n_mels=num_spectrogram_bins,).T\n","    # Apply the filter to the spectrogram\n","    mel_spectrograms = np.tensordot(stfts,linear_to_mel_weight_matrix,1)\n","    return np.log(mel_spectrograms + 1e-6)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YlTKMq1xSkhk"},"outputs":[],"source":["def plot_logMelSpectrogram(audio, fe, dt=0.025):\n","    sns.heatmap(np.rot90(logMelSpectrogram(audio, fe, dt)), cmap='inferno', vmin = -6)\n","    loc, labels = plt.xticks()\n","    l = np.round((loc-loc.min())*len(audio)/fe/loc.max(), 2)\n","    plt.xticks(loc, l)\n","    plt.yticks([])\n","    plt.xlabel(\"Time (s)\")\n","    plt.ylabel(\"Frequency (Mel)\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fkF6RIkiSkhl"},"outputs":[],"source":["def wave_path(machine,folder,type_audio,k):\n","    machine_path='./'+machine+'/'\n","    type_path=folder+'/'\n","    zeros='00000000'+str(k)\n","    file_name=type_audio+'_id_00_'+zeros[-8:]+'.wav'\n","    file_path=machine_path+type_path+file_name\n","    rate,data = wave.read(file_path)\n","    graph_name=file_name.split('.')[0]+'_spectre.jpg'\n","    save_path=machine_path+'Spectres/'+graph_name\n","    n = data.size\n","    duree = 1.0*n/rate\n","    return file_path,save_path,n,duree,rate,data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4kh-98GYSkhm"},"outputs":[],"source":["def tracerSpectre_op(data,rate,debut,duree):\n","    start = int(debut*rate)\n","    stop = int((debut+duree)*rate)\n","    # Récupération de la valeur absolue de la transformée de fourier sur l'échantillon\n","    spectre = np.absolute(fft(data[start:stop]))\n","    # normalisation des valeurs 0/1\n","    spectre = spectre/spectre.max()\n","    # On crée les labels des abscices \n","    n = spectre.size\n","    freq=np.linspace(1/n,1,n)*rate\n","    # On trace un barplot avec x=freq,y=spectre\n","    plt.vlines(freq,[0],spectre,'r')\n","    plt.xlabel('f (Hz)')\n","    plt.ylabel('A')\n","    plt.axis([0,0.5*rate,0,1])\n","    plt.grid()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6XkUbhYfSkhn"},"outputs":[],"source":["def Fourier(data,rate,debut,duree):\n","    start = int(debut*rate)\n","    stop = int((debut+duree)*rate)\n","    # Récupération de la valeur absolue de la transformée de fourier sur l'échantillon\n","    spectre = np.absolute(fft(data[start:stop]))\n","    # normalisation des valeurs 0/1\n","    spectre = spectre/spectre.max()\n","    # On crée les labels des abscices \n","    n = spectre.size\n","    freq=np.linspace(1/n,1,n)*rate\n","    return freq,spectre"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ANPIcIBnSkhn"},"outputs":[],"source":["def Frequences(data,rate):\n","    n = data.size\n","    duree = 1.0*n/rate\n","    start = int(0)\n","    stop = int(duree*rate)\n","    # Récupération de la valeur absolue de la transformée de fourier sur l'échantillon\n","    spectre = np.absolute(fft(data[start:stop]))\n","    # normalisation des valeurs 0/1\n","    spectre = spectre/spectre.max()\n","    return spectre"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H0ar5dyZSkho"},"outputs":[],"source":["def Dossier_data(DOSSIER):\n","    \"\"\"\n","    Cette fonction permet de parcourir les fichiers contenu \n","    dans le DOSSIER 'DOSSIER' QUE VOUS SOUHAITEZ du notebook d'en extraire\n","    les informations et de générer un PandaDataFrame\n","    \"\"\"\n","    if DOSSIER!='':\n","        chemin=glob.glob(''+DOSSIER+'/**',recursive=True)\n","    else :\n","        chemin=glob.glob('/**',recursive=True)\n","    \n","    df=pd.DataFrame(chemin, columns=['URL'])\n","    df['URL2']=df['URL'].apply(lambda contenu: os.path.split(contenu)[0])\n","    df['fichier']=df['URL'].apply(lambda contenu: os.path.split(contenu)[1])\n","    df['train_test']=df['URL2'].apply(lambda contenu: os.path.split(contenu)[1])\n","    df['URL2']=df['URL2'].apply(lambda contenu: os.path.split(contenu)[0])\n","    df['Machine']=df['URL2'].apply(lambda contenu: os.path.split(contenu)[1])\n","    df=df[(df['Machine']!='')&(df['Machine']!='.')&(df['Machine']!='Datascience_projet_son')&(df['Machine']!='./archive')&(df['URL2']!='./archive')]\n","    df['Machine_ID']=df['fichier'].apply(lambda contenu: contenu.split('_')[2])\n","    df['son_type']=df['fichier'].apply(lambda contenu: contenu.split('_')[0])\n","    df=df.drop('URL2',axis=1)\n","    duree=[]\n","    nb_boucles=df.shape[0]\n","    tps1 = time.time()\n","    for i,URL in enumerate(df['URL']):\n","      tps2 = time.time()\n","      print('boucle: ',i+1 ,' / ', nb_boucles+1)\n","      print('temps écoulé:{:.2f} sec'.format(tps2 - tps1))\n","      print('temps restant:{:.0f} min'.format((tps2 - tps1)*(nb_boucles-i)/((i+1)*60)))\n","      duree.append(duration(URL))\n","      clear_output()\n","      if i%1000==0 : # on supprime le cache de la RAM toutes les 1000 itérations\n","        librosa.cache.clear()\n","\n","    df['duree_son']=duree\n","    tps3 = time.time()\n","    print('temps total:',tps3 - tps1)\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kD3_tYOZSkho"},"outputs":[],"source":["def plot_loss(model_history,title=\"\"):\n","    plt.figure(figsize=(12,10))\n","    plt.plot(model_history.history['loss'][1:],'orange' ,label = 'Train Loss',)\n","    plt.plot(model_history.history['val_loss'][1:],'g', label = 'Test Loss')\n","    plt.title(title + \" , \"+ machine+\" \" +str(dt) +\" \" + str(freq))\n","    plt.ylabel('loss')\n","    plt.xlabel('epoch')\n","    plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W9mvmrq8Skho"},"outputs":[],"source":["def plot_mse_with_predict(model_load):\n","    pred_3d=model_load.predict(data_valid)\n","    pred_3d=pred_3d.reshape(-1, dt, freq)\n","\n","    #on calcule la MSE\n","    from sklearn.metrics import mean_squared_error\n","    from scipy import stats\n","    score_3D=[]\n","    with tqdm(total=y_norm_valid.shape[0]) as pbar:\n","        for pred, true in zip(pred_3d,data_valid):\n","            score_3D.append(mean_squared_error(true,pred))\n","            pbar.update(1)\n","\n","    score_3D_graph=pd.DataFrame(y_norm_valid)\n","    score_3D_graph['MSE']=np.array(score_3D)\n","    score_3D_graph.columns=['son_type','MSE']\n","    print(stats.ttest_ind(score_3D_graph[score_3D_graph.son_type=='anomaly']['MSE'],score_3D_graph[score_3D_graph.son_type=='normal']['MSE']))\n","\n","    score_3D_graph[score_3D_graph.son_type=='normal'].plot.hist(alpha=0.5)\n","    plt.title('normal')\n","    score_3D_graph[score_3D_graph.son_type=='anomaly'].plot.hist(alpha=0.5)\n","    plt.title('anomaly');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgylnDwZSkhp"},"outputs":[],"source":["def rebuild_heatmap():\n","    nb_to_plot=6\n","    select=[np.random.random(nb_to_plot)*y_norm_valid.shape[0]]\n","    select=np.array(select).astype('int').reshape(nb_to_plot)\n","    plt.figure(figsize=(20,4))\n","    for i in range (1,nb_to_plot+1):\n","        ax1=plt.subplot(2,nb_to_plot,i)\n","        # ax1.imshow(data_valid_cut[select[i-1]])\n","        sns.heatmap(data_valid[select[i-1]])\n","        plt.title(y_norm_valid[select[i-1]] + ' original ')\n","        ax1.get_yaxis().set_visible(False)\n","        ax1.get_xaxis().set_visible(False)\n","        ax2=plt.subplot(2,nb_to_plot,i+nb_to_plot)\n","        # ax2.imshow(pred_3d[select[i-1]])\n","        sns.heatmap(pred_3d[select[i-1]])\n","        plt.title(y_norm_valid[select[i-1]] + ' reconstitution ')\n","        ax2.get_yaxis().set_visible(False)\n","        ax2.get_xaxis().set_visible(False)\n","        # plt.legend()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hZ2cIcKfSkhp"},"outputs":[],"source":["def visualisation(model_history,model_load=None,title=\"\",noHeatmap=True):\n","\n","    if model_history is not None:\n","        plot_loss(model_history,title)\n","    if noHeatmap:\n","        rebuild_heatmap()\n","    if model_load is not None: \n","        plot_mse_with_predict(model_load)\n","    \n","    \n","\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"functions_utils.ipynb","provenance":[]},"interpreter":{"hash":"6d8a557c81f1e0fde6dab329f0421469b7b112dd74fe52052266c597b09c2f69"},"kernelspec":{"display_name":"Python 3.8.8 64-bit ('plaidml': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"metadata":{"interpreter":{"hash":"1baa965d5efe3ac65b79dfc60c0d706280b1da80fedb7760faf2759126c4f253"}}},"nbformat":4,"nbformat_minor":0}